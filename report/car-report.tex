\documentclass[12pt]{article}

\usepackage{fullpage}
\usepackage{cite}
\usepackage{multirow}
\usepackage{graphicx}


\title{Learning Visual Driving}
\author{Joel Iventosch and Ginevra Gaudioso and Josiah Hanna}
\date{}

\begin{document}

\maketitle

%********************************************************************************************************%
% (1) Introduction
\section{Introduction}
The aim of this work is to let the f1tenth race car drive on its own in our building floors. The f1tenth car is the small autonomous car used in the f1tenth competition (f1tenth.org). For the purpose of navigation, the car is equipped with a lidar sensor and a camera. The lidar sensor is used to determine the distance from objects or walls in front and to the sides of the car. The camera is used as a validation for the lidar, confirming or blocking the lidar decisions. 

The existing method proposes to only use lidar for navigation. The control algorithm is a Proportional Integral Derivative (PID) based controller that functions by constantly computing the distance to the wall to the right of the car, and adjusts the steering to keep a constant distance relative to the wall. It follows that the car will drive in clock-wise circles, as allowed by the building corridors. However, the existing method has several critical faults, since a strictly lidar-based PID control algorithm causes the car turn any time an apparent opening is detected to its right. This problem manifests itself in several settings:
\begin{enumerate}
\item Some doors have a metal shielding on the bottom, where the lidar signal is reflected: the car will detect an opening on the right while in reality there is a door
\item Not all openings on the right are actually accessible: in our building, certain floors have a kitchen area ad a copy room area. As soon as the car approaches these areas the lidar detects an opening (since the lidar's range is relatively limited), thus the PID controller will make the car turn right, resulting in the car crashing in the copy room or kitchen area. 
\end{enumerate}

This suggests two general problems with only using the lidar for navigation: 
\begin{itemize}
\item The lidar reading can be faulty
\item The lidar cannot distinguish an opening from a corridor
\end{itemize}

In order to solve the aftormentioned problems, we propose a method that uses the camera as a validation mechanism for the lidar readings. In fact, a camera facing in front of the car can distinguish certain key features in the environment, such as the overhead lights and patterns on the floor among others, and thus our proposed algorithm can determine whether to turn in the opening detected by the lidar or not based on the presence or absence of these distinguishing features. For instance, the kitchen is in the middle of the corridor: the camera will see that the corridor still continues ahead, and the control algorithm can ignore the opening detected by the lidar, allowing the car to keep driving straight. A similar scenario holds for the copy room and the metal shielded doors. However, when we are approaching the end of the corridor, and the lidar detects an opening on the right, then the camera will see that the car is nearing the end of the corridor, and thus the camera would leave the control to the lidar, allowing the car to make the appropriate PID adjustments to complete the turn to the right.

In our method, the camera is not used as a primary source of navigation, but rather as a source of validation. When the lidar detects an opening, the camera is used to decide whether to follow the lidar control (through the existing PID control algorithm), or whether to keep driving straight. Our proposed method addresses the problems highlighted earlier. Namely, those scenarios in which the lidar dictates a turn when the desired action is to continue forward down the hallway. 
%********************************************************************************************************%
%********************************************************************************************************%
% (2) Related Work (if we end up needing/wanting this...might not need it...or very short if we do have it)
\section{Related Work}
%********************************************************************************************************%
%********************************************************************************************************%
% (3) Methodology and Results (since our results section would be short anyways - and since we tried so many different approaches with varying results, it seems easier to have results in same section as methodology and just discuss results of each attempted methodology right after we discuss it in its own subsubsection, rather than having to revisit each method again in a different stand-alone Results section...open to other thoughts/suggestions though...?
\section{Methodology and Results}
% (3.1) Overview
\subsection{Overview}
% steal some material from Introduction section for this part...
We propose a method that modifies the exisiting PID-based controller by augmenting the control algorithm with a layer of vision-based intelligence. In our methodology, this additional intelligence layer enables the controller to determine when to trust and when to ignore (or appropriately modify) its PID directive that is computed from the lidar sensor readings. To accomplish this we design a vision-based classification system that outputs binary decisions to the main control algorithm. Thus the key to our proposed contribution is the design and implementation of a fast and accurate vision-based classifier. To perform classification we designed, implemented, and experimented with seven variations of classification  models, which can be roughly divided into two categories: \textbf{\textit{learned classifiers}} based on convolutional neural network (CNN) models, and \textbf{\textit{hand-engineered classifiers}} based on \textbf{E}nvironment-\textbf{S}pecific \textbf{I}ndicative \textbf{F}eature \textbf{(ESIF)} Detection. In the following subsections we motivate each approach and provide algorithmic details, results, and limitations of both categories of classification models.

% (3.2) Learned Classification: CNNs
\subsection{Learned Classification: CNNs}
% (3.2.1) Motivation
\subsubsection{Motivation}
There are many possible environments in which an autonomous agent (robot, car, etc.) might operate, each of which is likely to vary to some extent. Some environments might be drastically different - e.g. an autonomous factory robot operating in an industrial setting versus an autonomous race car operating on a track - while others might have a high degree of similarity - e.g. an autonomous F1tenth race car driving around GDC 3rd floor south wing versus GDC 6th floor north wing. In the former example it is obvious to see that drastically different environments will have dramatically different visual features (e.g. assembly line in factory versus track and stands in a racing environment). However, it is not as obvious in the latter example that the similar environments would have different visual features. However, from the perspective of the autonomous agent - at the pixel-level - even very similar environments can result in significantly different visual features. Since our proposed system relies on the autonomous agent making classification decisions based on visual cues in its environment, this is a nontrivial issue. 
\par It is precisely this issue - namely, the observation that even apparently similar operating environments can have very different visual features - that motivates our first category of classification models: learned neural network models. Neural Networks have long been known for their ability to learn predictive tasks with a high degree of accuracy when trained on substantial amounts of data [need ref]. Furthermore, in the past decade advances in nueral network architectures (i.e. convolutional neural networks (CNNs)) have enabled the design and implementation of extremely successful vision-based object classificaition systems that require no hand-engineering of features, but simply learn the predictive task based on an implicit (learned) feature representation of the data [need ref]. This internal representation is embedded into the model in the form of the model's weights (often referred to as ``parameters" in the literature, not to be confused with ``hyperparameters", which are those settings used to train neural models and much fewer in the number - e.g. millions of parameters compared to 1-10 hyperparameters typically) through the process of training the model [need ref]. One of the key findings in the image classification literature is that CNNs are able to learn a very useful heirarchy of feature representations by training simply on raw pixel data. 
\par Returning to the task of autonomous control of our race car, the high variablity in visual features in the environment make CNNs a great fit for the classification task required by our proposed method. Rather than using hand-engineered rules to identify useful features in the environment that can then be used for classification, this category of classifiers instead proposes to learn the features in the enironment implicitly by training a CNN to make a binary classification decision (whether the car should turn or continue forward) based on the current (forward-looking) image of its environment collected from the camera.

% (3.2.2) Data Collection and Preprocessing
\subsubsection{Data Collection and Preprocessing}
In order to train the CNN classifier, we collected visual data in different floors of GDC. We placed the camera pointing forward on the car, and started recording. We pushed the car around the floor, following the intended path (the path that does not turn into the copy room or in the kitchen, but just follows the corridors). We collected 10 laps of video overall, in five different floors and two different driving directions (clockwise and counter-clockwise). We collected the data in different times of the day, to ensure that the data included different light exposures, thus enabling the classifier to be more robust to different environmental factors and to generalize better to slightly differing environments.
Each lap consisted of approximately 30-45 seconds of video, collected at 10 Hz. We fragmented the videos into frames, resulting in a total of 3602 frames. Each fram was assigned a label out of the set ${T,S,Bad}$. Label $T$ was used in these frames corresponding to positions in which the car should have turned. This means that when seeing frames similar to these labeled $T$, the control algorithm should follow the lidar reading and turn when an opening is detected. Label $S$ was used in these frames corresponding to positions in which the car should not have turned. The intuition behind our method of data collection and labelling is that when seeing frames similar to these labeled $S$ at run time, the control algorithm should not follow the lidar (in the case the lidar detected an opening) but rather keep going straight. Label $Bad$ was used for faulty frames. The classifier was trained on the frames with labels $T$ and $S$, and the frames with label $Bad$ were discarded. This resulted in approximately 3000 frames of traning data.
\par The training data was preprocessed using standard preprocessing methods, with the exception that no mean pixel was subtracted. Given the relatively small size of the data set, we thought this might hurt generalization purformance during test (run) time. The data was preprocessed by:
\begin{enumerate}
\item Each image resized and downsampled to 224x224x3 (height, width, channels)
\item Each image converted from RBG (red, blue, green) to GBR in the channels dimension, to conform to the data format that was used to train the VGG models that we used to initiate our model parameters (see section \ref{sec:model_specs} for details on our model architectures)
\item At run time, image frame collected from the camera are preprocessed in the same manner - thus the computational complexity of the preprocessing operation (per image) is an important factor.
\item To avoid imbalance, random instances of frames corresponding to $T$ labels were duplicated until the data set was approximately balanced.
\item The data was randomly split into a training set (85\%), validation set (10\%) and test set (5\%)
\item All models (discussed below) were trained on the training set, using the validation set to monitor the model's performance at the end of each trainining epoch. At the end of training, each model was tested on the held-out test set to determine final performance metric when deciding which model we might use in the final control system.
\end{enumerate}

% (3.2.3) CNN Model Specifications
\subsubsection{CNN Model Specifications} \label{sec:model_specs}
We experimented with five different model architectures, all of which were variants of the popular and successful VGG CNN architecture [VGG ref]. Furthermore, each of our proposed models used the pretrained weights from the VGG16 model trained by [VGG ref] for the 2012 (???) ImageNet Challenge, for one or more layers in our model. The layers in our models that were initated with the VGG pretrained weights were frozen - meaning the parameters for those layers in the model were not updated during training. The layers in our various models that did not use pretrained weights were initiated using small random values drawn from a zero mean Guassian distribution with a standard deviation of 0.1. In choosing our model architectures, we considered several key factors:
\begin{itemize}
\item \textbf{speed:} to fuction in the real-time enviroment of the autonomous F1tenth race car, we needed our model to be fast enough to operate at a high enough frequency that it could provide timely classification decisions to the master control algorithm. We estimated that we would need our classifier to run at approximately five to ten Hz in order for the control algorithm to function properly - thus whatever model architecture we used had to be able to complete a forward pass through the network (including preprocessing operations) in 100-200 milliseconds. Given that the full VGG16 (``16" represents 16 different layers in the architecture) takes approxiately 400 milliseconds on a multicore CPU (measured experimentally), not including preprocessing, most of our proposed model architectures are shallower (less layers) than the full VGG16 model, and thus much faster.
\item \textbf{accuracy:} for our modified control algorithm to function properly, our model proposed model must be accurate enough to provide reliable classification decisions. This poses a challenge, since speed and accuracy are often inversely related. Fortunatlely, the binary classificaation task at hand is simple enough that the best of our proposed models achieves both concurrently.
\item \textbf{generalization/overfitting:} Given the relatively small size of our training data set (approximately 3000 images versus over one million images in the ImageNet data set), we were concerned about our model overfitting the training data, thus suffering from poor generalization performance at run time and in different environments (e.g. GDC 3rd floor south wing versus 6th floor south wing)
 \end{itemize}
Given the above considerations, we proposed the following four model architectures for use in our experiments. Each was trained using the same training procedure: the Adam variant of stochastic gradient descent [Adam ref], trained for 250 epochs or until convergence (all models, except vgg16, converged well before 250 epochs). All models were trained using the same training and validation data sets, and then tested on the same held-out test set. Model specifications and results are shown in Table \ref{table:model_specs} and Table \ref{table:model_results} respectively.
 
% Please add the following required packages to your document preamble:
% \usepackage{multirow}
% \usepackage{graphicx}
\begin{table}[]
\centering
\caption{CNN Model Architectures}
\label{table:model_specs}
\resizebox{\textwidth}{!}{%
\begin{tabular}{|r|c|l|l|l|l|l|l|l|l|}
\hline
\multicolumn{1}{|c|}{\multirow{2}{*}{\textbf{\begin{tabular}[c]{@{}c@{}}model\\ name\end{tabular}}}} & \multirow{2}{*}{\textbf{\begin{tabular}[c]{@{}c@{}}\# layers\\ (with\\ weights)\end{tabular}}} & \multicolumn{7}{c|}{\textbf{architecture description}}                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        & \multicolumn{1}{c|}{\multirow{2}{*}{\textbf{Notes}}}                                                                                                                                                                                        \\ \cline{3-9}
\multicolumn{1}{|c|}{}                                                                               &                                                                                                & \multicolumn{1}{c|}{\textit{layer name}}                                                                                                                                                                                                                                    & \multicolumn{1}{c|}{\textit{operation}}                                                                                                                                                                                                                                                                                                                                                                                                                                                        & \multicolumn{1}{c|}{\textit{\begin{tabular}[c]{@{}c@{}}frozen (pre-trained) /\\ trainable weights\end{tabular}}}                                                                                                                           & \multicolumn{1}{c|}{\textit{\begin{tabular}[c]{@{}c@{}}filter\\ size\end{tabular}}}                                                                    & \multicolumn{1}{c|}{\textit{\# filters}}                                                                                                                & \multicolumn{1}{c|}{\textit{stride}}                                                                                                                                   & \multicolumn{1}{c|}{\textit{\# params}}                                                                                                                                                                                                               & \multicolumn{1}{c|}{}                                                                                                                                                                                                                       \\ \hline
vgg16                                                                                                & 16                                                                                             & \multicolumn{1}{r|}{\begin{tabular}[c]{@{}r@{}}conv1\_1\\ conv1\_2\\ pool1\\ conv2\_1\\ conv2\_2\\ pool2\\ conv3\_1\\ conv3\_2\\ conv3\_3\\ pool3\\ conv4\_1\\ conv4\_2\\ conv4\_3\\ pool4\\ conv5\_1\\ conv5\_2\\ conv5\_3\\ pool5\\ fc1\\ fc2\\ fc3\\ TOTAL\end{tabular}} & \begin{tabular}[c]{@{}l@{}}convolution+ReLU\\ convolution+ReLU \\ max pooling \\ convolution+ReLU\\ convolution+ReLU \\ max pooling \\ convolution+ReLU\\ convolution+ReLU\\ convolution+ReLU \\ max pooling \\ convolution+ReLU\\ convolution+ReLU\\ convolution+ReLU \\ max pooling \\ convolution+ReLU\\ convolution+ReLU\\ convolution+ReLU \\ max pooling \\ fully connected+ReLU\\ fully connected+ReLU\\ fully connected+softmax\\ .......................................\end{tabular} & \begin{tabular}[c]{@{}l@{}}frozen\\ frozen\\ \\ frozen\\ frozen\\ \\ frozen\\ frozen\\ frozen\\ \\ frozen\\ frozen\\ frozen\\ \\ frozen\\ frozen\\ frozen\\ \\ frozen\\ frozen\\ trainable\\ ................................\end{tabular} & \begin{tabular}[c]{@{}l@{}}3x3\\ 3x3 \\ \\ 3x3\\ 3x3\\ \\ 3x3\\ 3x3\\ 3x3\\ \\ 3x3\\ 3x3\\ 3x3\\ \\ 3x3\\ 3x3\\ 3x3\\ \\ \\ \\ \\ .......\end{tabular} & \begin{tabular}[c]{@{}l@{}}64\\ 64\\ \\ \\ 128\\ 128\\ \\ 256\\ 256\\ 256\\ \\ 512\\ 512\\ 512\\ \\ 512\\ 512\\ 512\\ \\ \\ \\ ...........\end{tabular} & \begin{tabular}[c]{@{}l@{}}1x1\\ 1x1\\ 2x2\\ 1x1\\ 1x1\\ 2x2\\ 1x1\\ 1x1\\ 1x1\\ 2x2\\ 1x1\\ 1x1\\ 1x1\\ 2x2\\ 1x1\\ 1x1\\ 1x1\\ 2x2\\ \\ \\ \\ .........\end{tabular} & \begin{tabular}[c]{@{}l@{}}1,728\\ 36,864\\ 0\\ 73,728\\ 147,456\\ 0\\ 294,912\\ 589,824\\ 589,824\\ 0\\ 1,1179,648\\ 2,359,296\\ 2,359,296\\ 0\\ 2,359,296\\ 2,359,296\\ 2,359,296\\ 0\\ 102,760,448\\ 16,777,216\\ 8,192\\ 144,256,320\end{tabular} & \begin{tabular}[c]{@{}l@{}}only 8,192\\ trainable wghts\\ but still slow due\\ to depth of arch.\end{tabular}                                                                                                                               \\ \hline
vgg10                                                                                                & 10                                                                                             & \begin{tabular}[c]{@{}l@{}}conv1\_1\\ conv1\_2\\ pool1\\ conv2\_1\\ conv2\_2\\ pool2\\ conv3\_1\\ conv3\_2\\ conv3\_3\\ pool3\\ fc1\\ fc2\\ fc3\\ TOTAL\end{tabular}                                                                                                        & \begin{tabular}[c]{@{}l@{}}convolution+ReLU\\ convolution+ReLU \\ max pooling \\ convolution+ReLU\\ convolution+ReLU \\ max pooling \\ convolution+ReLU\\ convolution+ReLU\\ convolution+ReLU \\ max pooling \\ fully connected+ReLU\\ fully connected+ReLU\\ fully connected+softmax\\ ......................................\end{tabular}                                                                                                                                                    & \begin{tabular}[c]{@{}l@{}}frozen\\ frozen\\ \\ frozen\\ frozen\\ \\ frozen\\ frozen\\ frozen\\ \\ trainable\\ trainable\\ trainable\\ ................................\end{tabular}                                                       & \begin{tabular}[c]{@{}l@{}}3x3\\ 3x3\\ \\ 3x3\\ 3x3\\ \\ 3x3\\ 3x3\\ 3x3\\ \\ \\ \\ \\ ......\end{tabular}                                             & \begin{tabular}[c]{@{}l@{}}64\\ 64\\ \\ 128\\ 128\\ \\ 256\\ 256\\ 256\\ \\ \\ \\ \\ ..........\end{tabular}                                            & \begin{tabular}[c]{@{}l@{}}1x1\\ 1x1\\ 2x2\\ 1x1\\ 1x1\\ 2x2\\ 1x1\\ 1x1\\ 1x1\\ 2x2\\ \\ \\ \\ ........\end{tabular}                                                  & \begin{tabular}[c]{@{}l@{}}1,728\\ 36,864\\ 0\\ 73,728\\ 147,456\\ 0\\ 294,912\\ 589,824\\ 589,824\\ 0\\ 20,070,400\\ 10,000\\ 200\\ 21,814,936\end{tabular}                                                                                          & \begin{tabular}[c]{@{}l@{}}20,080,600 trainable\\ wghts, but faster than\\ vgg16 bc of \\ shallower arch\end{tabular}                                                                                                                       \\ \hline
vgg6t                                                                                                & 6                                                                                              & \begin{tabular}[c]{@{}l@{}}conv1\_1\\ pool1\\ conv2\_1\\ pool2\\ conv3\_1\\ pool3\\ fc1\\ fc2\\ fc3\\ TOTAL\end{tabular}                                                                                                                                                    & \begin{tabular}[c]{@{}l@{}}convolution+ReLU \\ max pooling \\ convolution+ReLU \\ max pooling \\ convolution+ReLU \\ max pooling \\ fully connected+ReLU \\ fully connected+ReLU \\ fully connected+softmax\\ ......................................\end{tabular}                                                                                                                                                                                                                              & \begin{tabular}[c]{@{}l@{}}frozen\\ trainable\\ \\ trainable\\ \\ trainable\\ trainable\\ \\ trainable\\ ...............................\end{tabular}                                                                                      & \begin{tabular}[c]{@{}l@{}}3x3\\ \\ 3x3\\ \\ 3x3\\ \\ \\ \\ \\ ......\end{tabular}                                                                     & \begin{tabular}[c]{@{}l@{}}64\\ \\ 128\\ \\ 128\\ \\ \\ \\ \\ ..........\end{tabular}                                                                   & \begin{tabular}[c]{@{}l@{}}1x1\\ 2x2\\ 1x1\\ 2x2\\ 1x1\\ 2x2\\ \\ \\ \\ .......\end{tabular}                                                                           & \begin{tabular}[c]{@{}l@{}}1,728\\ 0\\ 73,728\\ 0\\ 147,456\\ 0\\ 10,035,200\\ 10,000\\ 200\\ 10,268,312\end{tabular}                                                                                                                                 & \begin{tabular}[c]{@{}l@{}}10,266,584 trainable\\ wghts (99.98\% of all\\ wghts) yet still best\\ performance on test\\ set (see results table\\ for details). Also\\ faster since shallower\\ architecture.\end{tabular}                   \\ \hline
vgg6f                                                                                                & 6                                                                                              & \begin{tabular}[c]{@{}l@{}}conv1\_1\\ pool1\\ conv2\_1\\ pool2\\ conv3\_1\\ pool3\\ fc1\\ fc2\\ fc3\\ TOTAL\end{tabular}                                                                                                                                                    & \begin{tabular}[c]{@{}l@{}}convolution+ReLU \\ max pooling \\ convolution+ReLU \\ max pooling \\ convolution+ReLU \\ max pooling \\ fully connected+ReLU \\ fully connected+ReLU \\ fully connected+softmax\\ ......................................\end{tabular}                                                                                                                                                                                                                              & \begin{tabular}[c]{@{}l@{}}frozen\\ \\ frozen\\ \\ frozen\\ \\ trainable\\ trainable\\ trainable\\ ...............................\end{tabular}                                                                                            & \begin{tabular}[c]{@{}l@{}}3x3\\ \\ 3x3\\ \\ 3x3\\ \\ \\ \\ \\ .....\end{tabular}                                                                      & \begin{tabular}[c]{@{}l@{}}64\\ \\ 128\\ \\ 256\\ \\ \\ \\ \\ ..........\end{tabular}                                                                   & \begin{tabular}[c]{@{}l@{}}1x1\\ 2x2\\ 1x1\\ 2x2\\ 1x1\\ 2x2\\ \\ \\ \\ .......\end{tabular}                                                                           & \begin{tabular}[c]{@{}l@{}}1,728\\ 0\\ 73,728\\ 0\\ 294,912\\ 0\\ 20,070,400\\ 10,000\\ 200\\ 20,450,968\end{tabular}                                                                                                                                 & \begin{tabular}[c]{@{}l@{}}20,080,600 trainable\\ wghts (98.19\% of all\\ wghts).\\ had to keep conv3\_1\\ larger since using\\ pre-trained (frozen)\\ weights.\\ slightly slower than\\ vgg6t model and \\ generalizes worse.\end{tabular} \\ \hline
\end{tabular}%
}
\end{table}
% (3.2.4) Results
\subsubsection{Results}
% Please add the following required packages to your document preamble:
% \usepackage{multirow}
% \usepackage{graphicx}
\begin{table}[]
\centering
\caption{CNN Model Results}
\label{table:model_results}
\resizebox{\textwidth}{!}{%
\begin{tabular}{|r|c|c|c|c|c|l|}
\hline
\multicolumn{1}{|c|}{\multirow{2}{*}{\textbf{\begin{tabular}[c]{@{}c@{}}model\\ name\end{tabular}}}} & \multicolumn{5}{c|}{\textbf{Results}}                                                                                                                                                                                                             & \multicolumn{1}{c|}{\multirow{2}{*}{\textbf{Notes}}}                                                                                                                             \\ \cline{2-6}
\multicolumn{1}{|c|}{}                                                                               & \textit{Train Acc} & \textit{Val Acc} & \textit{Test Acc} & \textit{Train Time}                                                             & \textit{\begin{tabular}[c]{@{}c@{}}Forward Pass\\ not incl. preproc.\\ (CPU/GPU est.)\end{tabular}} & \multicolumn{1}{c|}{}                                                                                                                                                            \\ \hline
vgg16                                                                                                & 92.2\%             & 81.3\%           & 85.96 \%          & $\sim$5 hrs                                                                     & 803 ms / 462 ms                                                                                     & \begin{tabular}[c]{@{}l@{}}250 epochs of training.\\ 153/178 on test set.\end{tabular}                                                                                           \\ \hline
vgg10                                                                                                & 100.0\%            & 96.9\%           & 95.51\%           & $\sim$2.25 hrs                                                                  & 415 ms / 237 ms                                                                                     & \begin{tabular}[c]{@{}l@{}}159 epochs of training.\\ 170/178 on test set.\\ clearly overfit (too many params).\end{tabular}                                                      \\ \hline
vgg6t                                                                                                & 100.0\%            & 99.2\%           & 97.19\%           & $\sim$1.5 hrs                                                                   & 113 ms / 65 ms                                                                                      & \begin{tabular}[c]{@{}l@{}}57 epochs of training.\\ 173 /178 on test set.\end{tabular}                                                                                           \\ \hline
vgg6f                                                                                                & 95.3\%             & 92.9\%           & 93.82\%           & \begin{tabular}[c]{@{}c@{}}$\sim$4.75 hrs\\ (for all 250\\ epochs)\end{tabular} & 142 ms / 82 ms                                                                                      & \begin{tabular}[c]{@{}l@{}}25 epochs of training.\\ displayed very strange oscillating\\ performance on train and val set\\ during training.\\ 167/178 on test set.\end{tabular} \\ \hline
\end{tabular}%
}
\end{table}


% (3.2.5) Benefits and Limitations
\subsubsection{Benefits and Limitations}


% (3.3) Hand-Engineered Classification: Environment-Specific Indicative Feature (ESIF) Detection
\subsection{Hand-Engineered Classification: ESIF Detection}
% Environment-Specific Indicative Feature (ESIF) Detection

% (3.3.1) Motivation
\subsubsection{Motivation}

% (3.3.2) Indicative Features
\subsubsection{Indicative Features}

% (3.2.3) ESIF Algorithm
\subsubsection{ESIF Algorithm}
% TODO:Jo , can you work on this part a bit when you have a chance? In particular, the algorithmic details about how you guys did:
% (1) the ``scan" method (btw, was that after thresholding?)
% (2) the ``blob" method from robocup code (was that after thresholding, or did we not threshold when using this method?)
% also what should we use as a reference for the robocup code? Thanks!



% (3.3.4) Results
\subsubsection{Results}

% (3.3.5) Benefits and Limitations
\subsubsection{Benefits and Limitations}




% (4) Challenges and Setbacks:
\section{Challenges and Setbacks:}
Working with the car presented several challenges that overall slowed down our progress. However, we always tried to develop a solution for these issues, and we think that overall we made a great contribution to the car project as a whole.

First, we encountered some architectural issues with the on board GPU. Our Neural Network classifier worked perfectly on one of our laptops, achieving 97\% accuracy levels. However, the only Tensor Flow library available for that GPU was a significantly lower version than the current one, and thus we had to translate all our code into a older version, losing speed and accuracy. Overall, the older version still worked in one of our laptops, but for some reason it did not work properly on the GPU. We tried to debug the code, and concluded that there were some architectural incompatibilities, and thus we were forced to momentarily abandon the Neural Network approach. However, we proved that in general such deep learning method can be used to process visual input and output correct driving decisions. Our Neural Network method still remains valid, and we think it should be considered as a flexible and robust solution to the driving problem.

Once we moved to the heuristic method, we run into more hardware issues with the car itself.
First, the car seems to drift significantly to the left when no angle command is given. The inability for the car to go straight without a constant wall reference on the right, as detected by the lidar, required us to come up with another navigation guideline to follow in blind areas such as the kitchen. We proposed a method to follow the red strip on the carpet on the left of the corridor, using visual input. The method can become as robust as the current ``wall on the right" approach: the car can follow both lines, the wall and the red strip, and consequently follow the hallway more easily: in case the wall on the right goes missing, like in the kitchen, the strip on the left can be used. The redundancy of driving guides included in our method can make navigation significantly more robust.

There were some issues that we were not fully able to solve, but that indeed contributed in slowing down our progress. One such issue is that the on board battery has a very short life, and the car board can only remain powered for a little more than an hour, reducing any test time to a few tests. Moreover, it did happen that the on board battery did not turn on, making it impossible for us to work on the car. Another issue we faced is that sometimes the main program would lose contact with the lidar and the camera. We do not know if there is a hardware problem, such as wiring, or if there is a software problem in detecting the hardware. While this experience clearly taught us that failures do occur in real life, and that it is important to build fault tolerant systems, this experience also made it impossible for us to use the car for some significant time. Lastly, we had some difficulties in tuning the PID parameters at different speeds. Unfortunately, the motors would go at different speeds based on how much the motor battery is charged, even if the speed inputted from software is the same. We tuned PID parameters at one speed, and then while keeping the same software speed the PID parameters won?t work again after the battery discharged a little bit.

Overall, we are proposing several methods to overcome some of the issues we faced with the car, and our methods can be integrated and used in the whole car code, moving the whole project forward.

%********************************************************************************************************%
%********************************************************************************************************%
% (4) Discussion and Future Work
\section{Discussion and Future Work}

%********************************************************************************************************%
%********************************************************************************************************%


\section{Misc}

%i'm just sticking stuff here, will need to be moved when vision part is added.


%angle threshold part:
While we want to discard full turns in these cases where a turn is not desired (thus when a faulty or bad opening is detected by the lidar), we still want to keep the car running in the middle of the corridor. Thus, even when our image classifier dictates "Straight", we still want to allow minor adjustments. In order to do so, we established an angle threshold of 20°. If the turn command dictated by the pre-existing algorithm and the lidar reading was less than 20°, it was considered a minor adjustment, and thus it was allowed even if our classifier would state "Straight". However, turns greater than 20° were ignored. The value of 20° was established experimentally. 
%end of angle threshold part



\end{document}