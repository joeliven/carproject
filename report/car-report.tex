\documentclass[12pt]{article}

\usepackage{fullpage}
\usepackage{cite}


\title{Learning Visual Driving}
\author{Joel Iventosch and Ginevra Gaudioso and Josiah Hanna}
\date{}

\begin{document}

\maketitle

%********************************************************************************************************%
% (1) Introduction
\section{Introduction}
The aim of this work is to let the f1tenth race car drive on its own in our building floors. The f1tenth car is the small autonomous car used in the f1tenth competition (f1tenth.org). For the purpose of navigation, the car is equipped with a lidar sensor and a camera. The lidar sensor is used to determine the distance from objects or walls in front and to the sides of the car. The camera is used as a validation for the lidar, confirming or blocking the lidar decisions. 

The existing method proposes to only use lidar for navigation. The control algorithm is a Proportional Integral Derivative (PID) based controller that functions by constantly computing the distance to the wall to the right of the car, and adjusts the steering to keep a constant distance relative to the wall. It follows that the car will drive in clock-wise circles, as allowed by the building corridors. However, the existing method has several critical faults, since a strictly lidar-based PID control algorithm causes the car turn any time an apparent opening is detected to its right. This problem manifests itself in several settings:
\begin{enumerate}
\item Some doors have a metal shielding on the bottom, where the lidar signal is reflected: the car will detect an opening on the right while in reality there is a door
\item Not all openings on the right are actually accessible: in our building, certain floors have a kitchen area ad a copy room area. As soon as the car approaches these areas the lidar detects an opening (since the lidar's range is relatively limited), thus the PID controller will make the car turn right, resulting in the car crashing in the copy room or kitchen area. 
\end{enumerate}

This suggests two general problems with only using the lidar for navigation: 
\begin{itemize}
\item The lidar reading can be faulty
\item The lidar cannot distinguish an opening from a corridor
\end{itemize}

In order to solve the aftormentioned problems, we propose a method that uses the camera as a validation mechanism for the lidar readings. In fact, a camera facing in front of the car can distinguish certain key features in the environment, such as the overhead lights and patterns on the floor among others, and thus our proposed algorithm can determine whether to turn in the opening detected by the lidar or not based on the presence or absence of these distinguishing features. For instance, the kitchen is in the middle of the corridor: the camera will see that the corridor still continues ahead, and the control algorithm can ignore the opening detected by the lidar, allowing the car to keep driving straight. A similar scenario holds for the copy room and the metal shielded doors. However, when we are approaching the end of the corridor, and the lidar detects an opening on the right, then the camera will see that the car is nearing the end of the corridor, and thus the camera would leave the control to the lidar, allowing the car to make the appropriate PID adjustments to complete the turn to the right.

In our method, the camera is not used as a primary source of navigation, but rather as a source of validation. When the lidar detects an opening, the camera is used to decide whether to follow the lidar control (through the existing PID control algorithm), or whether to keep driving straight. Our proposed method addresses the problems highlighted earlier. Namely, those scenarios in which the lidar dictates a turn when the desired action is to continue forward down the hallway. 
%********************************************************************************************************%
%********************************************************************************************************%
% (2) Related Work (if we end up needing/wanting this...might not need it...or very short if we do have it)
\section{Related Work}
%********************************************************************************************************%
%********************************************************************************************************%
% (3) Methodology and Results (since our results section would be short anyways - and since we tried so many different approaches with varying results, it seems easier to have results in same section as methodology and just discuss results of each attempted methodology right after we discuss it in its own subsubsection, rather than having to revisit each method again in a different stand-alone Results section...open to other thoughts/suggestions though...?
\section{Methodology and Results}
% (3.1) Overview
\subsection{Overview}
% steal some material from Introduction section for this part...
We propose a method that modifies the exisiting PID-based controller by augmenting the control algorithm with a layer of vision-based intelligence. In our methodology, this additional intelligence layer enables the controller to determine when to trust and when to ignore (or appropriately modify) its PID directive that is computed from the lidar sensor readings. To accomplish this we design a vision-based classification system that outputs binary decisions to the main control algorithm. Thus the key to our proposed contribution is the design and implementation of a fast and accurate vision-based classifier. To perform classification we designed, implemented, and experimented with eight variations of classification  models, which can be roughly divided into two categories: \textbf{\textit{learned classifiers}} based on convolutional neural network (CNN) models, and \textbf{\textit{hand-engineered classifiers}} based on \textbf{E}nvironment-\textbf{S}pecific \textbf{I}ndicative \textbf{F}eature \textbf{(ESIF)} Detection. In the following subsections we motivate each approach and provide algorithmic details, results, and limitations of both categories of classification models.

% (3.2) Learned Classification: CNNs
\subsection{Learned Classification: CNNs}
% (3.2.1) Motivation
\subsubsection{Motivation}
There are many possible environments in which an autonomous agent (robot, car, etc.) might operate, each of which is likely to vary to some extent. Some environments might be drastically different - e.g. an autonomous factory robot operating in an industrial setting versus an autonomous race car operating on a track - while others might have a high degree of similarity - e.g. an autonomous F1tenth race car driving around GDC 3rd floor south wing versus GDC 6th floor north wing. In the former example it is obvious to see that drastically different environments will have dramatically different visual features (e.g. assembly line in factory versus track and stands in a racing environment). However, it is not as obvious in the latter example that the similar environments would have different visual features. However, from the perspective of the autonomous agent - at the pixel-level - even very similar environments can result in significantly different visual features. Since our proposed system relies on the autonomous agent making classification decisions based on visual cues in its environment, this is a nontrivial issue. 
\par It is precisely this issue - namely, the observation that even apparently similar operating environments can have very different visual features - that motivates our first category of classification models: learned neural network models. Neural Networks have long been known for their ability to learn predictive tasks with a high degree of accuracy when trained on substantial amounts of data [need ref]. Furthermore, in the past decade advances in nueral network architectures (i.e. convolutional neural networks (CNNs)) have enabled the design and implementation of extremely successful vision-based object classificaition systems that require no hand-engineering of features, but simply learn the predictive task based on an implicit (learned) feature representation of the data [need ref]. This internal representation is embedded into the model in the form of the model's weights (often referred to as ``parameters" in the literature, not to be confused with ``hyperparameters", which are those settings used to train neural models and much fewer in the number - e.g. millions of parameters compared to 1-10 hyperparameters typically) through the process of training the model [need ref]. One of the key findings in the image classification literature is that CNNs are able to learn a very useful heirarchy of feature representations by training simply on raw pixel data. 
\par Returning to the task of autonomous control of our race car, the high variablity in visual features in the environment make CNNs a great fit for the classification task required by our proposed method. Rather than using hand-engineered rules to identify useful features in the environment that can then be used for classification, this category of classifiers instead proposes to learn the features in the enironment implicitly by training a CNN to make a binary classification decision (whether the car should turn or continue forward) based on the current (forward-looking) image of its environment collected from the camera.

% (3.2.2) Data Collection and Preprocessing
\subsubsection{Data Collection and Preprocessing}
In order to train the CNN classifier, we collected visual data in different floors of GDC. We placed the camera pointing forward on the car, and started recording. We pushed the car around the floor, following the intended path (the path that does not turn into the copy room or in the kitchen, but just follows the corridors). We collected 10 laps of video overall, in five different floors and two different driving directions (clockwise and counter-clockwise). We collected the data in different times of the day, to ensure that the data included different light exposures, thus enabling the classifier to be more robust to different environmental factors and to generalize better to slightly differing environments.
Each lap consisted of approximately 30-45 seconds of video, collected at 10 Hz. We fragmented the videos into frames, resulting in a total of 3602 frames. Each fram was assigned a label out of the set ${T,S,Bad}$. Label $T$ was used in these frames corresponding to positions in which the car should have turned. This means that when seeing frames similar to these labeled $T$, the control algorithm should follow the lidar reading and turn when an opening is detected. Label $S$ was used in these frames corresponding to positions in which the car should not have turned. The intuition behind our method of data collection and labelling is that when seeing frames similar to these labeled $S$ at run time, the control algorithm should not follow the lidar (in the case the lidar detected an opening) but rather keep going straight. Label $Bad$ was used for faulty frames. The classifier was trained on the frames with labels $T$ and $S$, and the frames with label $Bad$ were discarded. This resulted in approximately 3000 frames of traning data.
\par The training data was preprocessed using standard preprocessing methods, with the exception that no mean pixel was subtracted. Given the relatively small size of the data set, we thought this might hurt generalization purformance during test (run) time. The data was preprocessed by:
\begin{enumerate}
\item Each image resized and downsampled to 224x224x3 (height, width, channels)
\item Each image converted from RBG (red, blue, green) to GBR in the channels dimension, to conform to the data format that was used to train the VGG models that we used to initiate our model parameters (see section \ref{sec:vggmodels} for details on our model architectures)
\item At run time, image frame collected from the camera are preprocessed in the same manner - thus the computational complexity of the preprocessing operation (per image) is an important factor.
\item To avoid imbalance, random instances of frames corresponding to $T$ labels were duplicated until the data set was approximately balanced.
\item The data was randomly split into a training set (85\%), validation set (10\%) and test set (5\%)
\item All models (discussed below) were trained on the training set, using the validation set to monitor the model's performance at the end of each trainining epoch. At the end of training, each model was tested on the held-out test set to determine final performance metric when deciding which model we might use in the final control system.
\end{enumerate}

% (3.2.3) CNN Model Specifications
\subsubsection{CNN Model Specifications}
We experimented with five different model architectures, all of which were variants of the popular and successful VGG CNN architecture [VGG ref]. Furthermore, each of our proposed models used the pretrained weights from the VGG16 model trained by [VGG ref] for the 2012 (???) ImageNet Challenge, for one or more layers in our model. The layers in our models that were initated with the VGG pretrained weights were frozen - meaning the parameters for those layers in the model were not updated during training. The layers in our various models that did not use pretrained weights were initiated using small random values drawn from a zero mean Guassian distribution with a standard deviation of 0.1. In choosing our model architectures, we considered several key factors:
\begin{itemize}
\item \textbf{speed:} to fuction in the real-time enviroment of the autonomous F1tenth race car, we needed our model to be fast enough to operate at a high enough frequency that it could provide timely classification decisions to the master control algorithm. We estimated that we would need our classifier to run at approximately five to ten Hz in order for the control algorithm to function properly - thus whatever model architecture we used had to be able to complete a forward pass through the network (including preprocessing operations) in 100-200 milliseconds. Given that the full VGG16 (``16" represents 16 different layers in the architecture) takes approxiately 400 milliseconds on a multicore CPU (measured experimentally), not including preprocessing, most of our proposed model architectures are shallower (less layers) than the full VGG16 model, and thus much faster.
\item \textbf{accuracy:} for our modified control algorithm to function properly, our model proposed model must be accurate enough to provide reliable classification decisions. This poses a challenge, since speed and accuracy are often inversely related. Fortunatlely, the binary classificaation task at hand is simple enough that the best of our proposed models achieves both concurrently.
\item \textbf{generalization/overfitting:} Given the relatively small size of our training data set (approximately 3000 images versus over one million images in the ImageNet data set), we were concerned about our model overfitting the training data, thus suffering from poor generalization performance at run time and in different environments (e.g. GDC 3rd floor south wing versus 6th floor south wing)
 \end{itemize}
 Given the above considerations, we proposed the following five model architectures for use in our experiments. Each was trained using the same training procedure: the Adam variant of stochastic gradient descent [Adam ref], trained for 250 epochs or until convergence (all models converged well before 250 epochs). All models were trained using the same training and validation data sets, and then tested on the same held-out test set. Model specifications and results are shown in Table \ref{table:modelspecs} and Table \ref{table:modelresults} respectively.
 
 RIGHT HERE

% (3.2.4) Results
\subsubsection{Results}

% (3.2.5) Benefits and Limitations
\subsubsection{Benefits and Limitations}


% (3.3) Hand-Engineered Classification: Environment-Specific Indicative Feature (ESIF) Detection
\subsection{Hand-Engineered Classification: ESIF Detection}
% Environment-Specific Indicative Feature (ESIF) Detection

% (3.3.1) Motivation
\subsubsection{Motivation}

% (3.3.2) Indicative Features
\subsubsection{Indicative Features}

% (3.2.3) ESIF Algorithm
\subsubsection{ESIF Algorithm}



% (3.3.4) Results
\subsubsection{Results}

% (3.3.5) Benefits and Limitations
\subsubsection{Benefits and Limitations}




% (3.4) Coping with Faulty Harware:
\subsection{Coping with Faulty Harware:}

%********************************************************************************************************%
%********************************************************************************************************%
% (4) Discussion and Future Work
\section{Discussion and Future Work}

%********************************************************************************************************%
%********************************************************************************************************%

%i'm just sticking stuff here, will need to be moved when vision part is added.


%angle threshold part:
While we want to discard full turns in these cases where a turn is not desired (thus when a faulty or bad opening is detected by the lidar), we still want to keep the car running in the middle of the corridor. Thus, even when our image classifier dictates "Straight", we still want to allow minor adjustments. In order to do so, we established an angle threshold of 20°. If the turn command dictated by the pre-existing algorithm and the lidar reading was less than 20°, it was considered a minor adjustment, and thus it was allowed even if our classifier would state "Straight". However, turns greater than 20° were ignored. The value of 20° was established experimentally. 
%end of angle threshold part



\section{Results and Discussion}


\section{Related Works}

% MPC for helicopter flight
% GPS End-to-end learning

\section{Outlook and Conclusion}


\end{document}