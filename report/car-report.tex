\documentclass[12pt]{article}

\usepackage{fullpage}
\usepackage{cite}
\usepackage{multirow}
\usepackage{graphicx}
\usepackage[]{algorithm2e}


\title{Learning Visual Driving}
\author{Joel Iventosch and Ginevra Gaudioso and Josiah Hanna}
\date{}

\begin{document}

\maketitle

%********************************************************************************************************%
% (1) Introduction
\section{Introduction} \label{sec:intro}
The aim of this work is to let the f1tenth race car drive on its own in our building floors. The f1tenth car is the small autonomous car used in the f1tenth competition (f1tenth.org). For the purpose of navigation, the car is equipped with a lidar sensor and a camera. The lidar sensor is used to determine the distance from objects or walls in front and to the sides of the car. The camera is used as a validation for the lidar, confirming or blocking the lidar decisions. 

The existing method proposes to only use lidar for navigation. The control algorithm is a Proportional Integral Derivative (PID) based controller that functions by constantly computing the distance to the wall to the right of the car, and adjusts the steering to keep a constant distance relative to the wall. It follows that the car will drive in clock-wise circles, as allowed by the building corridors. However, the existing method has several critical faults, since a strictly lidar-based PID control algorithm causes the car turn any time an apparent opening is detected to its right. This problem manifests itself in several settings:
\begin{enumerate}
\item Some doors have a metal shielding on the bottom, where the lidar signal is reflected: the car will detect an opening on the right while in reality there is a door
\item Not all openings on the right are actually accessible: in our building, certain floors have a kitchen area ad a copy room area. As soon as the car approaches these areas the lidar detects an opening (since the lidar's range is relatively limited), thus the PID controller will make the car turn right, resulting in the car crashing in the copy room or kitchen area. 
\end{enumerate}

This suggests two general problems with only using the lidar for navigation: 
\begin{itemize}
\item The lidar reading can be faulty
\item The lidar cannot distinguish an opening from a corridor
\end{itemize}

In order to solve the aforementioned problems, we propose a method that uses the camera as a validation mechanism for the lidar readings. In fact, a camera facing in front of the car can distinguish certain key features in the environment, such as the overhead lights and patterns on the floor among others, and thus our proposed algorithm can determine whether to turn in the opening detected by the lidar or not based on the presence or absence of these distinguishing features. For instance, the kitchen is in the middle of the corridor: the camera will see that the corridor still continues ahead, and the control algorithm can ignore the opening detected by the lidar, allowing the car to keep driving straight. A similar scenario holds for the copy room and the metal shielded doors. However, when we are approaching the end of the corridor, and the lidar detects an opening on the right, then the camera will see that the car is nearing the end of the corridor, and thus the camera would leave the control to the lidar, allowing the car to make the appropriate PID adjustments to complete the turn to the right.

In our method, the camera is not used as a primary source of navigation, but rather as a source of validation. When the lidar detects an opening, the camera is used to decide whether to follow the lidar control (through the existing PID control algorithm), or whether to keep driving straight. Our proposed method addresses the problems highlighted earlier. Namely, those scenarios in which the lidar dictates a turn when the desired action is to continue forward down the hallway. 

The remainder of this paper is organized as follows (sections pertaining to the actual method we successfully deployed on the car are noted in \textbf{bold} below and preceded by \textbf{***}):
\begin{itemize}
\item Section 2.1: presents an overview of our approach(es)
\item \textbf{***Section 2.2: outlines our Control Algorithm}
\item Section 2.3: provides a detailed analysis of our Learned Classification approach
\item \textbf{***Section 2.4: provides a detailed analysis of our Hand-Engineered Classification approach}
\item Section 3: details some of the challenges and setbacks we encountered
\item Section 4: provides concluding thoughts
\end{itemize}
%********************************************************************************************************%
%********************************************************************************************************%
% (3) Methodology and Results (since our results section would be short anyways - and since we tried so many different approaches with varying results, it seems easier to have results in same section as methodology and just discuss results of each attempted methodology right after we discuss it in its own subsubsection, rather than having to revisit each method again in a different stand-alone Results section...open to other thoughts/suggestions though...?
\section{Methodology and Results} \label{sec:method_res}
% (3.1) Overview
\subsection{Overview} \label{sec:method_res_overview}
We propose a method that modifies the existing PID-based controller by augmenting the control algorithm with a layer of vision-based intelligence. In our methodology, this additional intelligence layer enables the controller to determine when to trust and when to ignore (or appropriately modify) its PID directive that is computed from the lidar sensor readings. To accomplish this we design a vision-based classification system that outputs binary decisions to the main control algorithm. Thus the key to our proposed contribution is the design and implementation of a fast and accurate vision-based classifier. To perform classification we designed, implemented, and experimented with seven variations of classification  models, which can be roughly divided into two categories: \textbf{\textit{learned classifiers}} based on convolutional neural network (CNN) models, and \textbf{\textit{hand-engineered classifiers}} based on \textbf{E}nvironment-\textbf{S}pecific \textbf{I}ndicative \textbf{F}eature \textbf{(ESIF)} Detection. In the following subsections we motivate each approach and provide algorithmic details, results, and limitations of both categories of classification models. \textbf{\textit{Although our efforts in designing and implementing the Learned Classifier approach are described in great detail, the approach that we ultimately used on the car - and what was used in practice during our demonstration of a successful lap - was our ESIF approach.}}

\subsection{Control Algorithm} \label{sec:control_algo}
% TODO:Jo , can you modify and add a little to this for the control algo section? Thanks!
While we want to discard full turns in these cases where a turn is not desired (thus when a faulty or bad opening is detected by the lidar), we still want to keep the car running in the middle of the corridor. Thus, even when our image classifier dictates "Straight", we still want to allow minor adjustments. In order to do so, we established an angle threshold of 20°. If the turn command dictated by the pre-existing algorithm and the lidar reading was less than 20°, it was considered a minor adjustment, and thus it was allowed even if our classifier would state "Straight". However, turns greater than 20° were ignored. The value of 20° was established experimentally. 

% (3.2) Learned Classification: CNNs
\subsection{Learned Classification: CNNs} \label{sec:cnns}
% (3.2.1) Motivation
\subsubsection{Motivation} \label{sec:cnns_motivation}
There are many possible environments in which an autonomous agent (robot, car, etc.) might operate, each of which is likely to vary to some extent. Some environments might be drastically different - e.g. an autonomous factory robot operating in an industrial setting versus an autonomous race car operating on a track - while others might have a high degree of similarity - e.g. an autonomous F1tenth race car driving around GDC 3rd floor south wing versus GDC 6th floor north wing. In the former example it is obvious to see that drastically different environments will have dramatically different visual features (e.g. assembly line in factory versus track and stands in a racing environment). However, it is not as obvious in the latter example that the similar environments would have different visual features. However, from the perspective of the autonomous agent - at the pixel-level - even very similar environments can result in significantly different visual features. Since our proposed system relies on the autonomous agent making classification decisions based on visual cues in its environment, this is a nontrivial issue. 
\par It is precisely this issue - namely, the observation that even apparently similar operating environments can have very different visual features - that motivates our first category of classification models: learned neural network models. Neural Networks have long been known for their ability to learn predictive tasks with a high degree of accuracy when trained on substantial amounts of data [need ref]. Furthermore, in the past decade advances in neural network architectures (i.e. convolutional neural networks (CNNs)) have enabled the design and implementation of extremely successful vision-based object classification systems that require no hand-engineering of features, but simply learn the predictive task based on an implicit (learned) feature representation of the data [need ref]. This internal representation is embedded into the model in the form of the model's weights (often referred to as ``parameters" in the literature, not to be confused with ``hyperparameters", which are those settings used to train neural models and much fewer in the number - e.g. millions of parameters compared to 1-10 hyperparameters typically) through the process of training the model [need ref]. One of the key findings in the image classification literature is that CNNs are able to learn a very useful hierarchy of feature representations by training simply on raw pixel data. 
\par Returning to the task of autonomous control of our race car, the high variability in visual features in the environment make CNNs a great fit for the classification task required by our proposed method. Rather than using hand-engineered rules to identify useful features in the environment that can then be used for classification, this category of classifiers instead proposes to learn the features in the environment implicitly by training a CNN to make a binary classification decision (whether the car should turn or continue forward) based on the current (forward-looking) image of its environment collected from the camera.

% (3.2.2) Data Collection and Preprocessing
\subsubsection{Data Collection and Preprocessing} \label{sec:cnns_data}
In order to train the CNN classifier, we collected visual data in different floors of GDC. We placed the camera pointing forward on the car, and started recording. We pushed the car around the floor, following the intended path (the path that does not turn into the copy room or in the kitchen, but just follows the corridors). We collected 10 laps of video overall, in five different floors and two different driving directions (clockwise and counter-clockwise). We collected the data in different times of the day, to ensure that the data included different light exposures, thus enabling the classifier to be more robust to different environmental factors and to generalize better to slightly differing environments.
Each lap consisted of approximately 30-45 seconds of video, collected at 10 Hz. We fragmented the videos into frames, resulting in a total of 3602 frames. Each frame was assigned a label out of the set ${T,S,Bad}$. Label $T$ was used in these frames corresponding to positions in which the car should have turned. This means that when seeing frames similar to these labeled $T$, the control algorithm should follow the lidar reading and turn when an opening is detected. Label $S$ was used in these frames corresponding to positions in which the car should not have turned. The intuition behind our method of data collection and labeling is that when seeing frames similar to these labeled $S$ at run time, the control algorithm should not follow the lidar (in the case the lidar detected an opening) but rather keep going straight. Label $Bad$ was used for faulty frames. The classifier was trained on the frames with labels $T$ and $S$, and the frames with label $Bad$ were discarded. This resulted in approximately 3000 frames of training data.
\par The training data was preprocessed using standard preprocessing methods, with the exception that no mean pixel was subtracted. Given the relatively small size of the data set, we thought this might hurt generalization performance during test (run) time. The data was preprocessed by:
\begin{enumerate}
\item Each image resized and downsampled to 224x224x3 (height, width, channels)
\item Each image converted from RBG (red, blue, green) to GBR in the channels dimension, to conform to the data format that was used to train the VGG models that we used to initiate our model parameters (see section \ref{sec:cnns_model_specs} for details on our model architectures)
\item At run time, image frame collected from the camera are preprocessed in the same manner - thus the computational complexity of the preprocessing operation (per image) is an important factor.
\item To avoid imbalance, random instances of frames corresponding to $T$ labels were duplicated until the data set was approximately balanced.
\item The data was randomly split into a training set (85\%), validation set (10\%) and test set (5\%)
\item All models (discussed below) were trained on the training set, using the validation set to monitor the model's performance at the end of each training epoch. At the end of training, each model was tested on the held-out test set to determine final performance metric when deciding which model we might use in the final control system.
\end{enumerate}

% (3.2.3) CNN Model Specifications
\subsubsection{CNN Model Specifications} \label{sec:cnns_model_specs}
Detailed specifications of each model are shown in Table \ref{table:model_specs} of Appendix \ref{appendixA}. We experimented with five different model architectures, all of which were variants of the popular and successful VGG CNN architecture [VGG ref]. Furthermore, each of our proposed models used the pretrained weights from the VGG16 model trained by [VGG ref] for the 2012 (???) ImageNet Challenge, for one or more layers in our model. The layers in our models that were initated with the VGG pretrained weights were frozen - meaning the parameters for those layers in the model were not updated during training. The layers in our various models that did not use pretrained weights were initiated using small random values drawn from a zero mean Guassian distribution with a standard deviation of 0.1. In choosing our model architectures, we considered several key factors:
\begin{itemize}
\item \textbf{speed:} to fuction in the real-time enviroment of the autonomous F1tenth race car, we needed our model to be fast enough to operate at a high enough frequency that it could provide timely classification decisions to the master control algorithm. We estimated that we would need our classifier to run at approximately five to ten Hz in order for the control algorithm to function properly - thus whatever model architecture we used had to be able to complete a forward pass through the network (including preprocessing operations) in 100-200 milliseconds. Given that the full VGG16 (``16" represents 16 different layers in the architecture) takes approxiately 400 milliseconds on a multicore CPU (measured experimentally), not including preprocessing, most of our proposed model architectures are shallower (less layers) than the full VGG16 model, and thus much faster.
\item \textbf{accuracy:} for our modified control algorithm to function properly, our model proposed model must be accurate enough to provide reliable classification decisions. This poses a challenge, since speed and accuracy are often inversely related. Fortunatlely, the binary classificaation task at hand is simple enough that the best of our proposed models achieves both concurrently.
\item \textbf{generalization/overfitting:} Given the relatively small size of our training data set (approximately 3000 images versus over one million images in the ImageNet data set), we were concerned about our model overfitting the training data, thus suffering from poor generalization performance at run time and in different environments (e.g. GDC 3rd floor south wing versus 6th floor south wing)
 \end{itemize}
Given the above considerations, we proposed the following four model architectures for use in our experiments. Each was trained using the same training procedure: the Adam variant of stochastic gradient descent [Adam ref], trained for 250 epochs or until convergence (all models, except vgg16, converged well before 250 epochs). All models were trained using the same training and validation data sets, and then tested on the same held-out test set.
 
% (3.2.4) Results
\subsubsection{Results} \label{sec:cnns_res}
Detailed results for each model are shown in Table \ref{table:model_results} of Appendix \ref{appendixA}. After training, each of the four proposed CNN models was tested on a small set of held-out data (178 images) that had not been used during training - neither for gradient updates nor validation. The intuition behind this was that we should use whichever model performed best on the test set in our master control algorithm (subject to speed requirements). Results for each model are detailed in table \ref{table:model_results}. It should be noted that the GPU computation times are very approximate estimates, because ultimately we were not able to sucessfully deploy the models on the specific GPU model onboard the car (Jeston TK1), due to hardware and software compatibility issues (see sections \ref{sec:cnns_bens_lims} and \ref{sec:challenges} for details). A few noticeable highlights were:
\begin{itemize}
\item All models were able to achieve \textit{relatively} good peformance
\item Vgg16 (arguably the most ``powerful" model) performed the worst. This can likely be attributed to the very low percentage of the weights that were not frozen (i.e. trainable) during training.
\item The shallower models (vgg6t, vgg6f) seemed to perform as well or better than the very deep models. This suggests that a \textit{binary} classification task of this nature is simple enough that an extremely deep representation is not necessary. In fact, we surmise that an even shallower network could likely acheive comparable performance on this task, while also being less prone to overfitting due to fewer overall weights (and thus less representational power). In fact, if a deep learning approach along these lines is considered for future work within the car reesarch group, we would suggest using a network similar to vgg6t, but shallower and with additional regularization pentalties applied during training to help reduce overfitting.
\item The shallower networks can classify at approxiately 10 Hz on a GPU (including preprocessing time), and are thus indeed fast enough to be used in this type of real-time control algorithm. This was a key finiding and is very encouraging for future work.
\item All of the models overfit the data, at least to some extent. In future work this could be addressed by using a shallower network, introducing additional regularization penalties, and training on a larger data set.
\end{itemize}

% (3.2.5) Benefits and Limitations
\subsubsection{Benefits and Limitations} \label{sec:cnns_bens_lims}
Here we summarize the key benefits and limitations of the overall \textbf{\textit{Learned Classification}} approach:\\
\textbf{BENEFITS}
\begin{itemize}
\item Hand-engineering of features not required.
\item More robust to subtle changes within a given environment (e.g. differences in lighting, time of day, visual interference from people, etc.) when trained properly.
\item Caple of generalizing across different environments (both subtly different, e.g. GDC 3rd south versus GDC 6th north and drastically different, e.g. GDC versus a completely different building) if trained properly and on a large enough and diverse enough data set
\item System is inherently capable of collecting training data. Binary labelling is tedious, but can be done in reasonable time.
\end{itemize}
\textbf{LIMITATIONS}
\begin{itemize}
\item Significant training time required.
\item Adjusting to completely new environments requires full retraining of model, not just recrafting hand-engineered features. Thus less flexiibility in making immediate or on-the-fly adjustments to control algorithm.
\item To train properly, hyparameters of the model might require tuning - this can be time consuming.
\item Not as fast as most hand-engineered control features at run-time.
\item Can be more difficult to debug because the feature representation is learned implicitly in the model - i.e. ``why is the network learning to focus on a particular part of the environment?" is a notoriously difficult question to answer.
\item Very difficult to implement on the car's current GPU hardware (Jetson TK1). The TK1 GPU does not support current versions of many of the libraries that are useful for neural network models. For instance, the TK1 appears to only support tensorflow 0.5 (current version is 0.12), at least to the best of our knowledge. Due to these hardware issues, although this approach appeared to be promising based on the aforementioned results, we were not actually able to make use of it in our final system. If this approach is desired as an avenue of future work within the research group, we suggest upgrading the GPU to the TX1 if possible, which appears to have much better support for neural network libraries.
\end{itemize}


% (3.3) Hand-Engineered Classification: Environment-Specific Indicative Feature (ESIF) Detection
\subsection{Hand-Engineered Classification: ESIF Detection} \label{sec:esif}
% Environment-Specific Indicative Feature (ESIF) Detection

% (3.3.1) Motivation
\subsubsection{Motivation} \label{sec:esif_motivation}
In spite of the promise of our proposed deep learning classification approach, the hardware-related implementation issues previously discussed rendered that approach infeasible. This motivated the search for alternative classfication techniqes that utilzed simpler software libraries that could be executed in the given hardware environment (namely the TK1 GPU). We observed that there were certain key distinguishing features in the environment in which we were operating - and in the absence of a machine learning approach to identify these features, we turned to a more traditional approach of feature detection through hand-engineered rules and algorithms. We call this approach: hand-engineered classification based on \textbf{E}nvironment-\textbf{S}pecific \textbf{I}ndicative \textbf{F}eature \textbf{(ESIF)} Detection. \textbf{\textit{To be clear, this approach - using our BLOB segmentation method in particular (details below) - is what we ultimately used on the car, and it was this approach that was in practice during our demonstration of a successful lap.}}


% (3.3.2) Indicative Features
\subsubsection{Indicative Features} \label{sec:esif_feats}
Our second category of proposed approaches to handling this control problem (ESIF) hinges on the identification of key distinguishing features in the specific operating environment that indicate when the classifier should allow the car to trust the control system's lidar reading and when it should ignore it. Once again, this essentially amounts to building a classifier that can detect when the car has reached the end of the coridor and should thus turn. To make the ESIF approach work, we first identified two environment-specific features that we could use:
\begin{itemize}
\item \textbf{\textit{Overhead light:}} The overhead strip lights in the GDC cooridor emit a very bright, high contrast, nearly white light that can be consistently identified within almost any image frame in the car's environment. The key intution is that when the car is in the long part of the cooridor (e.g. immediately after turning into the cooridor, near the kitchen, etc.) a significant portion of the overhead strip light is visible to the car, resulting in a long column of nearly white pixels running roughly through the center of the image. However, as the car nears the end of the cooridor (where it should make the right hand turn), much less of the overhead light is in view, thus the column of light visibile in the image frame is significantly shorter. Thus, by thresholding the image (to isolate the nearly white light) and approximately segmenting this column of light from the image, we can sum over the values of the remaining pixels to determine how far along the cooridor the car is, and thus when it should execute the turn.
\item \textbf{\textit{Red Strip of Carpet:}} A second key indicative feature that our proposed ESIF approach uses is the wide strip of red carpet that lines the outter portion of the long cooridor in the GDC hallways. This is a more difficult segmentation task since it is a single channel segmentation task (high red values, lower blue and green values), and also because the contrast ratio of this feature (i.e. the intensity of the carpets red values versus other pixels in the image) is significantly less than that of the overhead light feature. Nonetheless, approximate segmentation is still possible, enabling our control algorithm to use the relative location of the red strip of carpet in the image as a noisy signal indicating how close the car is to the ``wall" on the left - where the term ``wall" here refers to the track boundary on the left side of the car, regardless of whether it is a physical wall or not (e.g. in the kitchen area there is not a physical wall to the left, thus our algorithm uses the noisy signal provided by the red carpet as an indicator of how \textit{centered} it is on the desired track.
\end{itemize}

% (3.2.3) ESIF Algorithm
\subsubsection{ESIF Algorithm} \label{sec:esif_algo}
% TODO:Jo , can you work on this part a bit when you have a chance? In particular, the algorithmic details about how you guys did:
% (1) the ``scan" method (btw, was that after thresholding?)
% (2) the ``blob" method from robocup code (was that after thresholding, or did we not threshold when using this method?)
% also what should we use as a reference for the robocup code? Thanks!
At a high level, all variations of our ESIF algorithm have the structure descirbed by Algorithm \ref{algo:ESIF_hi}.\\
\begin{algorithm}
 \KwData{one 480x640x3 raw image frame}
 \KwResult{binary classification decision: Turn/Straight	}
$lightThreshold = 100;$ //constant parameter...must be tuned empirically\\
$img = preprocessImage(Data);$\\
$thresholded = thresholdImage(img);$\\
$lightSegment = segmentImage(thresholded);$\\
$amountOfLight = sum(lightSegment);$\\
  \If{$amountOfLight < lightThreshold$}{
   \Return $Turn$\;
   }
   \Return $Straight$\;
 \caption{High Level Algorithm for ESIF Approaches} \label{algo:ESIF_hi}
\end{algorithm}
\newline
The various components within this algorithm are fairly straight forward, with the exception of the segmentation function.
\begin{itemize}
\item \textit{preprocessImage}: preprocesses the image according the preprocessing procedure described in \ref{sec:cnns_data}
\item \textit{thresholdImage}: thresholds the image to zero out all pixel values in the image except for those in which all three channels have a pixel value above a very high threshold. This has the effect of zeroing out everything in the image except for very bright, white light. We used a threshold value of $\approx 0.985$ but this value is subject to environment-specific tuning and adjustment.
\item \textit{segmentImage}: one of the key challenges with this approach is that the environment can contain various sources of bright white light other than the overhead light strip - particularly depending on the time of day. For instance, an open window in the middle of the day can cause a bright block of light lower in the image that would cause the results of Algorithm \ref{algo:ESIF_hi} to be very unreliable if the thresholded image were simply summed over with out first segmenting. However, by identifying the relative location of the light strip in the image and segmenting that out from the rest of the image before summing, our algorithm becomes much more robust to noise and environmental variables. However, image segmentation (even on a simple, thresholded image such as this) is not a trivial task. As such, we experimented with three different approaches to the image segmentation task, namely: (1) Blob segmentation (code borrowed from  \cite{james2000fast}), (2) Scanning segmentation, and (3) Linear Regression on Patches segmentation. \textbf{\textit{Ultimately, we ended up using the Blob segmentation method, because it proved to be the most robust to noise and variations in the environment, thus providing the most consistent results.}} Each method is described in detail below:
\end{itemize}
\textbf{BLOB Segmentation:}
Test citation for blob method \cite{james2000fast}.


\textbf{SCANNING Segmentation:}


\textbf{LINEAR REGRESSION ON PATCHES Segmentation:}
This method segments the image into patches of $patchHeight$x$patchWidth$ (a parameter specified as an argument to the function) and fits the pixels in each patch to a line of best fit using simple linear regression. The error of the actual image data is then computed and returned along with the slope and intercept of the line of best fit for the patch. The patch with either the lowest error, most vertical slope, or some combination is then returned as the image patch containing the overhead light strip. The intuition behind this method is that the light emitted from the overhead strip light forms a very straight line compared to light emitted by other sources (e.g. a window). Thus the patch containing the light should be able to fit its pixel data to a straight line with significantly less error than patches containing alternative sources of light. Furthermore, given the orientation of the light (i.e. running parallel to the direction of travel of the car down the cooridor), the slope of the line of best fit through overhead light, should be very close to vertical (i.e. a very large - in absolute value terms - slope). One attractive aspect of this method is its adjustable patch size (in both the x and y directions), and its speed. Linear regression has a closed form solution, so the line of best fit be determined very efficiently by just a few matrix operations. A downside to this method is that as the $patchWidth$ is reduced, its accuray is also likely to degrade, since all of the data in a very narrow patch can always be fit to a perfect vertical line, in the limit.



% (3.3.4) Results
\subsubsection{Results} \label{sec:esif_res}
With ESIF approach quantitative results are not readily obtainable. With enough time and resources we could have potentially run multiple trials with both approaches to obtain quantifiable success/failure rates for each approach, however given the natural time constraints of a class project, this was not feasible. That said, the qualitative results of this approach are readily available, since this was ultimately the approach that we implemented on the car and used in our demonstration of a successful lap around the cooridor. Given that we were ablel to successfully complete a lap, we declare this approach to be a success in qualitative terms. Furthermore, although it identifies indicative features in the environment, our approach does not rely at all on artifically placed features - in other words, all key features that we base our classification decision on are permanent fixtures of the environment (i.e. overhead light strip, red carpet). We feel that this is a key attribute of ESIF and speaks to the qualitative success of our approach. Videos of several trial runs can be viewed at this link: \textbf{http://www.cs.utexas.edu/~ginevra/car/}. Note that the results of these videos vary significantly, because these videos were taken while the design and parameter tuning of our approach was still being fine-tuned.

% (3.3.5) Benefits and Limitations
\subsubsection{Benefits and Limitations} \label{sec:esif_bens_lims}
Here we summarize the key benefits and limitations of the overall \textbf{\textit{ESIF}} approach:\\
\textbf{BENEFITS}
\begin{itemize}
\item Does not rely on features or objects artificially introduced to the environment.
\item No lengthy model training procedure required.
\item More flexiibility in making immediate or on-the-fly adjustments to control algorithm, since classification does not rely on a learned model that must be retrained.
\item Faster at runtime than Learned Classifier approach.
\item Easier to debug because the feature representation is explicitly designed by human engineers as opposed to learned implicitly by the model.
\item No sophisticated low-level optimization libraries required, thus easier to implement on the car's current GPU hardware.
\item No need for collecting and labelling large amounts of training data.
\end{itemize}
\textbf{LIMITATIONS}
\begin{itemize}
\item Feature representation is completely hand-designed, not learned.
\item Adjusting to completely new environments requires recrafting hand-engineered features.
\item Many hand-designed parameters that must be tuned for each environment. Even within the same environment, these parameters must often be tuned to specific environmental settigs (e.g. time of day).
\item Less robust to subtle changes within a given environment (e.g. differences in lighting, time of day, visual interference from people, etc.).
\item Not caple of generalizing across different environments.
\end{itemize}


% (4) Challenges and Setbacks:
\section{Challenges and Setbacks:} \label{sec:challenges}
Working with the car presented several challenges that overall slowed down our progress. However, we always developed a solution for these issues, and overall we made a great contribution to the car project as a whole.

First, we encountered some architectural issues with the on board GPU. Our Neural Network classifier worked perfectly on one of our laptops, achieving 97\% accuracy levels. However, the only Tensor Flow library available for the onboard GPU was a significantly lower version than the current one, and thus we had to translate all our code into a older version, losing speed and accuracy. Overall, the older version still worked in one of our laptops, but for some reason it did not work properly on the GPU. We tried to debug the code, and concluded that there were some architectural incompatibilities, and thus we were forced to momentarily abandon the Neural Network approach. However, we proved that in general such deep learning method can be used to process visual input and output correct driving decisions. Our Neural Network method still remains valid, and we think it should be considered as a flexible and robust solution to the driving problem.

Once we moved to the heuristic method, we run into some more hardware issues with the car itself.
First, the car seems to drift significantly to the left when no angle command is given. The inability for the car to go straight without a constant wall reference on the right, as detected by the lidar, required us to come up with another navigation guideline to follow in blind areas such as the kitchen. We proposed a method to follow the red strip on the carpet on the left of the corridor, using visual input. The addition of the red-strip method makes navigation more robust as a whole: the car can follow both lines, the wall on the right and the red strip on the left, and consequently follow the hallway more easily and reliably. In case the wall on the right goes missing, like in the kitchen area, the strip on the left can be used. The redundancy of driving guides included in our method makes navigation significantly more robust.

There were some issues that we were not fully able to solve, but that indeed contributed in slowing down our progress. One such issue is that the on board battery has a very short life, and the car board can only remain powered for a little more than an hour, reducing any test time to a few tests. Moreover, it did happen that the on board battery did not turn on, or the main program would lose contact with the lidar and the camera. We do not know if there is a hardware problem, such as wiring, or if there is a software problem in detecting the hardware. While this experience clearly taught us that failures do occur in real life, and that it is important to build fault tolerant systems, this experience also made it impossible for us to use the car for some time. Lastly, we had some difficulties in tuning the PID parameters at different speeds. Unfortunately, the motors would go at different speeds based on how much the motor battery is charged, even if the speed inputted from software is the same. We tuned PID parameters at one speed, and then while keeping the same software speed the PID parameters won?t work again after the battery discharged a little bit.
%********************************************************************************************************%
%********************************************************************************************************%
% (4) Conclusions
\section{Conclusions} \label{sec:conclusions}
Overall, we are proposing several methods to overcome some of the issues we faced with the car, and our methods can be integrated and used in the whole car code, moving the whole project forward. We were able to make the car drive autonomously using the heuristics method, as demonstrated during the ?demo? on Wednesday December 7th. Despite difficulties, we kept working hard and always found a solutions to our problems. We think that our functions can provide a great addition to the driving capabilities of the car.

We want to emphasize one more time that our method relies on some fixed features. We do not need to carefully place markers in some parts of the hallway. We do not need to cover up some parts of the hallway. Our method does not rely on for-the-purpose marked features, such as card boards or carefully positioned trashcans, but our method works with the hallway as is. The lightning on the top and the strip in the carpet are always there, and constant in every floor. No setup is needed for our method. We adjusted the car for the given environment, rather than adjusting the environment for the car. We chose the harder approach, and still succeeded.
%********************************************************************************************************%
%********************************************************************************************************%

\bibliographystyle{plain}
\bibliography{report}

\newpage
\appendix
\section{Appendix} \label{appendixA}

% Please add the following required packages to your document preamble:
% \usepackage{multirow}
% \usepackage{graphicx}
\begin{table}[h!]
\centering
\caption{CNN Model Results}
\label{table:model_results}
\resizebox{\textwidth}{!}{%
\begin{tabular}{|r|c|c|c|c|c|l|}
\hline
\multicolumn{1}{|c|}{\multirow{2}{*}{\textbf{\begin{tabular}[c]{@{}c@{}}model\\ name\end{tabular}}}} & \multicolumn{5}{c|}{\textbf{Results}}                                                                                                                                                                                                             & \multicolumn{1}{c|}{\multirow{2}{*}{\textbf{Notes}}}                                                                                                                             \\ \cline{2-6}
\multicolumn{1}{|c|}{}                                                                               & \textit{Train Acc} & \textit{Val Acc} & \textit{Test Acc} & \textit{Train Time}                                                             & \textit{\begin{tabular}[c]{@{}c@{}}Forward Pass\\ not incl. preproc.\\ (CPU/GPU est.)\end{tabular}} & \multicolumn{1}{c|}{}                                                                                                                                                            \\ \hline
vgg16                                                                                                & 92.2\%             & 81.3\%           & 85.96 \%          & $\sim$5 hrs                                                                     & 803 ms / 462 ms                                                                                     & \begin{tabular}[c]{@{}l@{}}250 epochs of training.\\ 153/178 on test set.\end{tabular}                                                                                           \\ \hline
vgg10                                                                                                & 100.0\%            & 96.9\%           & 95.51\%           & $\sim$2.25 hrs                                                                  & 415 ms / 237 ms                                                                                     & \begin{tabular}[c]{@{}l@{}}159 epochs of training.\\ 170/178 on test set.\\ clearly overfit (too many params).\end{tabular}                                                      \\ \hline
vgg6t                                                                                                & 100.0\%            & 99.2\%           & 97.19\%           & $\sim$1.5 hrs                                                                   & 113 ms / 65 ms                                                                                      & \begin{tabular}[c]{@{}l@{}}57 epochs of training.\\ 173 /178 on test set.\end{tabular}                                                                                           \\ \hline
vgg6f                                                                                                & 95.3\%             & 92.9\%           & 93.82\%           & \begin{tabular}[c]{@{}c@{}}$\sim$4.75 hrs\\ (for all 250\\ epochs)\end{tabular} & 142 ms / 82 ms                                                                                      & \begin{tabular}[c]{@{}l@{}}25 epochs of training.\\ displayed very strange oscillating\\ performance on train and val set\\ during training.\\ 167/178 on test set.\end{tabular} \\ \hline
\end{tabular}%
}
\end{table}

% Please add the following required packages to your document preamble:
% \usepackage{multirow}
% \usepackage{graphicx}
\begin{table}[]
\centering
\caption{CNN Model Architectures}
\label{table:model_specs}
\resizebox{\textwidth}{!}{%
\begin{tabular}{|r|c|l|l|l|l|l|l|l|l|}
\hline
\multicolumn{1}{|c|}{\multirow{2}{*}{\textbf{\begin{tabular}[c]{@{}c@{}}model\\ name\end{tabular}}}} & \multirow{2}{*}{\textbf{\begin{tabular}[c]{@{}c@{}}\# layers\\ (with\\ weights)\end{tabular}}} & \multicolumn{7}{c|}{\textbf{architecture description}}                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        & \multicolumn{1}{c|}{\multirow{2}{*}{\textbf{Notes}}}                                                                                                                                                                                        \\ \cline{3-9}
\multicolumn{1}{|c|}{}                                                                               &                                                                                                & \multicolumn{1}{c|}{\textit{layer name}}                                                                                                                                                                                                                                    & \multicolumn{1}{c|}{\textit{operation}}                                                                                                                                                                                                                                                                                                                                                                                                                                                        & \multicolumn{1}{c|}{\textit{\begin{tabular}[c]{@{}c@{}}frozen (pre-trained) /\\ trainable weights\end{tabular}}}                                                                                                                           & \multicolumn{1}{c|}{\textit{\begin{tabular}[c]{@{}c@{}}filter\\ size\end{tabular}}}                                                                    & \multicolumn{1}{c|}{\textit{\# filters}}                                                                                                                & \multicolumn{1}{c|}{\textit{stride}}                                                                                                                                   & \multicolumn{1}{c|}{\textit{\# params}}                                                                                                                                                                                                               & \multicolumn{1}{c|}{}                                                                                                                                                                                                                       \\ \hline
vgg16                                                                                                & 16                                                                                             & \multicolumn{1}{r|}{\begin{tabular}[c]{@{}r@{}}conv1\_1\\ conv1\_2\\ pool1\\ conv2\_1\\ conv2\_2\\ pool2\\ conv3\_1\\ conv3\_2\\ conv3\_3\\ pool3\\ conv4\_1\\ conv4\_2\\ conv4\_3\\ pool4\\ conv5\_1\\ conv5\_2\\ conv5\_3\\ pool5\\ fc1\\ fc2\\ fc3\\ TOTAL\end{tabular}} & \begin{tabular}[c]{@{}l@{}}convolution+ReLU\\ convolution+ReLU \\ max pooling \\ convolution+ReLU\\ convolution+ReLU \\ max pooling \\ convolution+ReLU\\ convolution+ReLU\\ convolution+ReLU \\ max pooling \\ convolution+ReLU\\ convolution+ReLU\\ convolution+ReLU \\ max pooling \\ convolution+ReLU\\ convolution+ReLU\\ convolution+ReLU \\ max pooling \\ fully connected+ReLU\\ fully connected+ReLU\\ fully connected+softmax\\ .......................................\end{tabular} & \begin{tabular}[c]{@{}l@{}}frozen\\ frozen\\ \\ frozen\\ frozen\\ \\ frozen\\ frozen\\ frozen\\ \\ frozen\\ frozen\\ frozen\\ \\ frozen\\ frozen\\ frozen\\ \\ frozen\\ frozen\\ trainable\\ ................................\end{tabular} & \begin{tabular}[c]{@{}l@{}}3x3\\ 3x3 \\ \\ 3x3\\ 3x3\\ \\ 3x3\\ 3x3\\ 3x3\\ \\ 3x3\\ 3x3\\ 3x3\\ \\ 3x3\\ 3x3\\ 3x3\\ \\ \\ \\ \\ .......\end{tabular} & \begin{tabular}[c]{@{}l@{}}64\\ 64\\ \\ \\ 128\\ 128\\ \\ 256\\ 256\\ 256\\ \\ 512\\ 512\\ 512\\ \\ 512\\ 512\\ 512\\ \\ \\ \\ ...........\end{tabular} & \begin{tabular}[c]{@{}l@{}}1x1\\ 1x1\\ 2x2\\ 1x1\\ 1x1\\ 2x2\\ 1x1\\ 1x1\\ 1x1\\ 2x2\\ 1x1\\ 1x1\\ 1x1\\ 2x2\\ 1x1\\ 1x1\\ 1x1\\ 2x2\\ \\ \\ \\ .........\end{tabular} & \begin{tabular}[c]{@{}l@{}}1,728\\ 36,864\\ 0\\ 73,728\\ 147,456\\ 0\\ 294,912\\ 589,824\\ 589,824\\ 0\\ 1,1179,648\\ 2,359,296\\ 2,359,296\\ 0\\ 2,359,296\\ 2,359,296\\ 2,359,296\\ 0\\ 102,760,448\\ 16,777,216\\ 8,192\\ 144,256,320\end{tabular} & \begin{tabular}[c]{@{}l@{}}only 8,192\\ trainable wghts\\ but still slow due\\ to depth of arch.\end{tabular}                                                                                                                               \\ \hline
vgg10                                                                                                & 10                                                                                             & \begin{tabular}[c]{@{}l@{}}conv1\_1\\ conv1\_2\\ pool1\\ conv2\_1\\ conv2\_2\\ pool2\\ conv3\_1\\ conv3\_2\\ conv3\_3\\ pool3\\ fc1\\ fc2\\ fc3\\ TOTAL\end{tabular}                                                                                                        & \begin{tabular}[c]{@{}l@{}}convolution+ReLU\\ convolution+ReLU \\ max pooling \\ convolution+ReLU\\ convolution+ReLU \\ max pooling \\ convolution+ReLU\\ convolution+ReLU\\ convolution+ReLU \\ max pooling \\ fully connected+ReLU\\ fully connected+ReLU\\ fully connected+softmax\\ ......................................\end{tabular}                                                                                                                                                    & \begin{tabular}[c]{@{}l@{}}frozen\\ frozen\\ \\ frozen\\ frozen\\ \\ frozen\\ frozen\\ frozen\\ \\ trainable\\ trainable\\ trainable\\ ................................\end{tabular}                                                       & \begin{tabular}[c]{@{}l@{}}3x3\\ 3x3\\ \\ 3x3\\ 3x3\\ \\ 3x3\\ 3x3\\ 3x3\\ \\ \\ \\ \\ ......\end{tabular}                                             & \begin{tabular}[c]{@{}l@{}}64\\ 64\\ \\ 128\\ 128\\ \\ 256\\ 256\\ 256\\ \\ \\ \\ \\ ..........\end{tabular}                                            & \begin{tabular}[c]{@{}l@{}}1x1\\ 1x1\\ 2x2\\ 1x1\\ 1x1\\ 2x2\\ 1x1\\ 1x1\\ 1x1\\ 2x2\\ \\ \\ \\ ........\end{tabular}                                                  & \begin{tabular}[c]{@{}l@{}}1,728\\ 36,864\\ 0\\ 73,728\\ 147,456\\ 0\\ 294,912\\ 589,824\\ 589,824\\ 0\\ 20,070,400\\ 10,000\\ 200\\ 21,814,936\end{tabular}                                                                                          & \begin{tabular}[c]{@{}l@{}}20,080,600 trainable\\ wghts, but faster than\\ vgg16 bc of \\ shallower arch\end{tabular}                                                                                                                       \\ \hline
vgg6t                                                                                                & 6                                                                                              & \begin{tabular}[c]{@{}l@{}}conv1\_1\\ pool1\\ conv2\_1\\ pool2\\ conv3\_1\\ pool3\\ fc1\\ fc2\\ fc3\\ TOTAL\end{tabular}                                                                                                                                                    & \begin{tabular}[c]{@{}l@{}}convolution+ReLU \\ max pooling \\ convolution+ReLU \\ max pooling \\ convolution+ReLU \\ max pooling \\ fully connected+ReLU \\ fully connected+ReLU \\ fully connected+softmax\\ ......................................\end{tabular}                                                                                                                                                                                                                              & \begin{tabular}[c]{@{}l@{}}frozen\\ trainable\\ \\ trainable\\ \\ trainable\\ trainable\\ \\ trainable\\ ...............................\end{tabular}                                                                                      & \begin{tabular}[c]{@{}l@{}}3x3\\ \\ 3x3\\ \\ 3x3\\ \\ \\ \\ \\ ......\end{tabular}                                                                     & \begin{tabular}[c]{@{}l@{}}64\\ \\ 128\\ \\ 128\\ \\ \\ \\ \\ ..........\end{tabular}                                                                   & \begin{tabular}[c]{@{}l@{}}1x1\\ 2x2\\ 1x1\\ 2x2\\ 1x1\\ 2x2\\ \\ \\ \\ .......\end{tabular}                                                                           & \begin{tabular}[c]{@{}l@{}}1,728\\ 0\\ 73,728\\ 0\\ 147,456\\ 0\\ 10,035,200\\ 10,000\\ 200\\ 10,268,312\end{tabular}                                                                                                                                 & \begin{tabular}[c]{@{}l@{}}10,266,584 trainable\\ wghts (99.98\% of all\\ wghts) yet still best\\ performance on test\\ set (see results table\\ for details). Also\\ faster since shallower\\ architecture.\end{tabular}                   \\ \hline
vgg6f                                                                                                & 6                                                                                              & \begin{tabular}[c]{@{}l@{}}conv1\_1\\ pool1\\ conv2\_1\\ pool2\\ conv3\_1\\ pool3\\ fc1\\ fc2\\ fc3\\ TOTAL\end{tabular}                                                                                                                                                    & \begin{tabular}[c]{@{}l@{}}convolution+ReLU \\ max pooling \\ convolution+ReLU \\ max pooling \\ convolution+ReLU \\ max pooling \\ fully connected+ReLU \\ fully connected+ReLU \\ fully connected+softmax\\ ......................................\end{tabular}                                                                                                                                                                                                                              & \begin{tabular}[c]{@{}l@{}}frozen\\ \\ frozen\\ \\ frozen\\ \\ trainable\\ trainable\\ trainable\\ ...............................\end{tabular}                                                                                            & \begin{tabular}[c]{@{}l@{}}3x3\\ \\ 3x3\\ \\ 3x3\\ \\ \\ \\ \\ .....\end{tabular}                                                                      & \begin{tabular}[c]{@{}l@{}}64\\ \\ 128\\ \\ 256\\ \\ \\ \\ \\ ..........\end{tabular}                                                                   & \begin{tabular}[c]{@{}l@{}}1x1\\ 2x2\\ 1x1\\ 2x2\\ 1x1\\ 2x2\\ \\ \\ \\ .......\end{tabular}                                                                           & \begin{tabular}[c]{@{}l@{}}1,728\\ 0\\ 73,728\\ 0\\ 294,912\\ 0\\ 20,070,400\\ 10,000\\ 200\\ 20,450,968\end{tabular}                                                                                                                                 & \begin{tabular}[c]{@{}l@{}}20,080,600 trainable\\ wghts (98.19\% of all\\ wghts).\\ had to keep conv3\_1\\ larger since using\\ pre-trained (frozen)\\ weights.\\ slightly slower than\\ vgg6t model and \\ generalizes worse.\end{tabular} \\ \hline
\end{tabular}%
}
\end{table}


\end{document}